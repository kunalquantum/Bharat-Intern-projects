{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079d0da6",
   "metadata": {},
   "source": [
    "# AI Resume Parser (Ntk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517a70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395d41",
   "metadata": {},
   "source": [
    "import nltk: This imports the NLTK library, which stands for Natural Language Toolkit. NLTK is a widely used library in Python for natural language processing tasks.\n",
    "\n",
    "from nltk.tokenize import word_tokenize: This line imports the word_tokenize function from the nltk.tokenize module. word_tokenize is used to split a text into individual words or tokens.\n",
    "\n",
    "from nltk.corpus import stopwords: This line imports the stopwords module from the nltk.corpus package. Stopwords are common words (e.g., \"a\", \"an\", \"the\") that are often removed from text as they do not carry significant meaning.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer: This line imports the WordNetLemmatizer class from the nltk.stem module. Lemmatization is the process of reducing words to their base or dictionary form (e.g., \"running\" to \"run\").\n",
    "\n",
    "import PyPDF2: This imports the PyPDF2 library, which provides functionality for working with PDF files in Python.\n",
    "\n",
    "import re: This imports the regular expression (regex) module in Python. Regular expressions are used for pattern matching and text manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8237f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fb58c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "This line downloads the \"punkt\" resource, which includes pre-trained models and data for tokenization. Tokenization is the process of breaking text into individual words or tokens. The \"punkt\" resource provides trained models for tokenizing text in multiple languages.\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "This line downloads the \"stopwords\" resource, which consists of commonly used words that are often removed from text during text processing tasks. These words, such as \"the,\" \"is,\" \"and,\" etc., typically do not carry significant meaning and can be disregarded in certain NLP (Natural Language Processing) applications.\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "This line downloads the \"wordnet\" resource, which is a large lexical database of English words. WordNet provides synsets (sets of synonyms) and definitions for words, along with relationships between words like hypernyms (superordinate terms) and hyponyms (subordinate terms). It is widely used in various NLP tasks such as word sense disambiguation, semantic similarity, and more.\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "This line downloads the \"omw-1.4\" resource, which stands for Open Multilingual WordNet. It is a multilingual extension of WordNet and provides synsets and word relationships for several languages other than English.\n",
    "\n",
    "By executing these nltk.download() statements, you ensure that the required NLTK resources are available for your code to work properly. This step is typically done once, at the beginning of a project, to download the necessary resources before utilizing NLTK functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94a3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the preprocessed text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa71dec",
   "metadata": {},
   "source": [
    "Tokenization: The text is tokenized using the word_tokenize function from the NLTK library. Tokenization is the process of splitting the text into individual words or tokens.\n",
    "\n",
    "Lowercasing: Each token is converted to lowercase using a list comprehension. This step helps in standardizing the text and treating words in a case-insensitive manner.\n",
    "\n",
    "Stopword removal: Stopwords are common words that do not carry much meaning and are often removed from the text to reduce noise. The code retrieves a set of stopwords for the English language using the stopwords.words('english') function from NLTK. It then filters out the stopwords from the tokenized text using another list comprehension.\n",
    "\n",
    "Lemmatization: Lemmatization is the process of reducing words to their base or root form. It helps in normalizing words that have the same meaning but different forms (e.g., \"running\" and \"ran\" both become \"run\"). The code initializes a WordNetLemmatizer object from the NLTK library and applies lemmatization to each token using a list comprehension.\n",
    "\n",
    "Return: The preprocessed tokens are returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80005e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47993e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        resume_text = \"\"\n",
    "        \n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            resume_text += page.extract_text()\n",
    "        \n",
    "        return resume_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb7753",
   "metadata": {},
   "source": [
    "The function opens the PDF file in binary mode using the open function with the mode 'rb' (read binary). The file is opened within a context manager, denoted by the with statement. This ensures that the file is properly closed after it's processed, even if an exception occurs.\n",
    "\n",
    "The PdfReader class from the PyPDF2 library is used to read the PDF file. It takes the file object as an argument and creates a PdfReader object named pdf_reader.\n",
    "\n",
    "The variable num_pages is assigned the number of pages in the PDF file, which is obtained by calling the len function on pdf_reader.pages. This gives the total count of pages in the PDF document.\n",
    "\n",
    "The variable resume_text is initialized as an empty string. It will be used to store the extracted text from the PDF.\n",
    "\n",
    "A loop is set up to iterate over each page in the PDF. It uses the range function to generate a sequence of page numbers from 0 to num_pages - 1. For each page, the code retrieves the corresponding Page object from pdf_reader.pages using the current page_number.\n",
    "\n",
    "The extract_text method is called on the Page object to extract the text content from that page. The extracted text is then concatenated with the existing resume_text string.\n",
    "\n",
    "After all the pages have been processed, the function returns the accumulated resume_text, which contains the extracted text from the entire PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a361fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Create frequency distributions for the tokens\n",
    "    freq_dist1 = nltk.FreqDist(text1)\n",
    "    print(freq_dist1)\n",
    "    freq_dist2 = nltk.FreqDist(text2)\n",
    "    print(freq_dist2)\n",
    "    \n",
    "    # Calculate the Jaccard similarity coefficient\n",
    "    common_tokens = set(text1).intersection(set(text2))\n",
    "    similarity = len(common_tokens) / (len(text1) + len(text2) - len(common_tokens))\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b218d",
   "metadata": {},
   "source": [
    "The function begins by creating frequency distributions for the tokens in text1 and text2 using the FreqDist class from the NLTK library. The FreqDist object counts the occurrences of each token in the text. The resulting frequency distributions are assigned to the variables freq_dist1 and freq_dist2 respectively. These frequency distributions are useful for later calculations and analysis.\n",
    "\n",
    "The code then prints the frequency distributions using print(freq_dist1) and print(freq_dist2). This can be helpful for understanding the token frequencies in each text, but it is not necessary for the similarity calculation.\n",
    "\n",
    "The Jaccard similarity coefficient is calculated next. The Jaccard similarity is a measure of the similarity between two sets, in this case, the sets of tokens in text1 and text2. The code first finds the set of common tokens by taking the intersection of the sets created from text1 and text2. The set() function is used to convert the lists of tokens into sets. The result is stored in the common_tokens variable.\n",
    "\n",
    "The similarity is then calculated using the formula: similarity = len(common_tokens) / (len(text1) + len(text2) - len(common_tokens)). The length of the common_tokens set represents the number of tokens that are common to both texts. The denominator is the total number of tokens in both texts, subtracted by the number of common tokens to avoid counting them twice. The resulting value represents the Jaccard similarity coefficient.\n",
    "\n",
    "Finally, the similarity value is returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "049f71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job descriptionfruitseller with communication skills\n",
      "Enter the ResumeProfessional Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job_description = input(\"Enter the job description\")\n",
    "resume_file_path =input(\"Enter the Resume\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2def3e",
   "metadata": {},
   "source": [
    "job_description = input(\"Enter the job description\"): This line displays the message \"Enter the job description\" to the user and waits for input. The user can enter a description of a job, such as the requirements, responsibilities, or qualifications. The inputted value is then stored in the job_description variable.\n",
    "\n",
    "resume_file_path = input(\"Enter the Resume\"): This line displays the message \"Enter the Resume\" to the user and waits for input. The user is expected to provide the path to a resume file, which is typically a document in a specific format (e.g., PDF, Word) containing their professional information, skills, and experience. The inputted value, representing the file path, is stored in the resume_file_path variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45add1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_job ..fetched...\n",
      "job_description fetched...\n",
      "precessing-resume... done...\n"
     ]
    }
   ],
   "source": [
    "preprocessed_job = preprocess_text(job_description)\n",
    "print(\"preprocessed_job ..fetched...\")\n",
    "print(\"job_description fetched...\")\n",
    "resume_text = extract_text_from_pdf(resume_file_path)\n",
    "preprocessed_resume = preprocess_text(resume_text)\n",
    "print(\"precessing-resume... done...\")\n",
    "skills_pattern = r\"(\\b[\\w\\s&]+)\\b\\s?(?:\\b[\\w\\s&]+?\\b\\s?){0,3}\\b(?:skills|proficient in|expert in|knowledge in)\\b\"\n",
    "experience_pattern = r\"(\\d+)\\s?(?:year[s]?)?\\s?(?:of)?\\s?(?:experience)?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387f5c0",
   "metadata": {},
   "source": [
    "preprocessed_job = preprocess_text(job_description): The preprocess_text function is called, passing the job_description variable as the input. This function preprocesses the text by tokenizing it, converting it to lowercase, removing stopwords, and lemmatizing the tokens. The preprocessed result is stored in the preprocessed_job variable.\n",
    "\n",
    "print(\"preprocessed_job ..fetched...\"): This line simply prints the message \"preprocessed_job ..fetched...\" to the console, indicating that the preprocessed job description has been obtained.\n",
    "\n",
    "print(\"job_description fetched...\"): This line prints the message \"job_description fetched...\" to the console, indicating that the original job description input has been obtained.\n",
    "\n",
    "resume_text = extract_text_from_pdf(resume_file_path): The extract_text_from_pdf function is called, passing the resume_file_path variable as the input. This function reads the content of the PDF file located at the specified path and extracts the text from it. The extracted text is stored in the resume_text variable.\n",
    "\n",
    "preprocessed_resume = preprocess_text(resume_text): The preprocess_text function is called, passing the resume_text variable as the input. This function preprocesses the resume text in a similar way to the job description, by tokenizing, lowercasing, removing stopwords, and lemmatizing the tokens. The preprocessed result is stored in the preprocessed_resume variable.\n",
    "\n",
    "print(\"precessing-resume... done...\"): This line prints the message \"precessing-resume... done...\" to the console, indicating that the preprocessing of the resume text is complete.\n",
    "\n",
    "skills_pattern = r\"(\\b[\\w\\s&]+)\\b\\s?(?:\\b[\\w\\s&]+?\\b\\s?){0,3}\\b(?:skills|proficient in|expert in|knowledge in)\\b\": This line defines a regular expression pattern stored in the skills_pattern variable. The pattern is designed to match skill-related phrases in a text, such as \"skills,\" \"proficient in,\" \"expert in,\" or \"knowledge in.\"\n",
    "\n",
    "experience_pattern = r\"(\\d+)\\s?(?:year[s]?)?\\s?(?:of)?\\s?(?:experience)?\": This line defines another regular expression pattern stored in the experience_pattern variable. The pattern is designed to match experience-related phrases in a text, such as \"3 years of experience.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd36b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiance.. fetched...\n",
      "Skills parsed..\n",
      "<FreqDist with 3 samples and 3 outcomes>\n",
      "<FreqDist with 157 samples and 276 outcomes>\n",
      "Percentage of Compentency for the role 0.007220216606498195\n",
      "The chosen candidate is: Candidate is not suitable\n"
     ]
    }
   ],
   "source": [
    "skills = re.findall(skills_pattern, resume_text, re.IGNORECASE)\n",
    "experience = re.findall(experience_pattern, resume_text, re.IGNORECASE)\n",
    "print(\"Experiance.. fetched...\")\n",
    "print(\"Skills parsed..\")\n",
    "similarity = calculate_similarity(preprocessed_job, preprocessed_resume)\n",
    "print(\"Percentage of Compentency for the role\",similarity)\n",
    "if similarity > 0.01:\n",
    "    chosen_candidate = \"Candidate is suitable\"\n",
    "else:\n",
    "    chosen_candidate = \"Candidate is not suitable\"\n",
    "\n",
    "print(\"The chosen candidate is:\", chosen_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584431e",
   "metadata": {},
   "source": [
    "skills = re.findall(skills_pattern, resume_text, re.IGNORECASE): The re.findall() function is called with the skills_pattern, resume_text, and the re.IGNORECASE flag. This function searches for all matches of the skills_pattern regular expression in the resume_text and returns a list of matched skills. The resulting list of skills is stored in the skills variable.\n",
    "\n",
    "experience = re.findall(experience_pattern, resume_text, re.IGNORECASE): Similar to the previous step, the re.findall() function is called with the experience_pattern, resume_text, and the re.IGNORECASE flag. This function searches for all matches of the experience_pattern regular expression in the resume_text and returns a list of matched experiences. The resulting list of experiences is stored in the experience variable.\n",
    "\n",
    "print(\"Experiance.. fetched...\"): This line prints the message \"Experiance.. fetched...\" to the console, indicating that the experiences from the resume text have been obtained.\n",
    "\n",
    "print(\"Skills parsed..\"): This line prints the message \"Skills parsed...\" to the console, indicating that the skills from the resume text have been parsed.\n",
    "\n",
    "similarity = calculate_similarity(preprocessed_job, preprocessed_resume): The calculate_similarity function is called with the preprocessed job description (preprocessed_job) and preprocessed resume text (preprocessed_resume) as inputs. This function calculates the similarity between the two texts using the Jaccard similarity coefficient, and the resulting similarity value is stored in the similarity variable.\n",
    "\n",
    "print(\"Percentage of Compentency for the role\", similarity): This line prints the message \"Percentage of Compentency for the role\" followed by the calculated similarity value to the console, providing information about the similarity between the job description and the resume.\n",
    "\n",
    "if similarity > 0.001:: This line starts an if statement, comparing the similarity value with 0.001. If the similarity is greater than this threshold, the following block of code will be executed.\n",
    "\n",
    "chosen_candidate = \"Candidate is suitable\": This line assigns the string \"Candidate is suitable\" to the chosen_candidate variable.\n",
    "\n",
    "else:: This line starts an else block, which will be executed if the condition in the if statement is not met.\n",
    "\n",
    "chosen_candidate = \"Candidate is not suitable\": This line assigns the string \"Candidate is not suitable\" to the chosen_candidate variable.\n",
    "\n",
    "print(\"The chosen candidate is:\", chosen_candidate): This line prints the message \"The chosen candidate is:\" followed by the value of the chosen_candidate variable, indicating whether the candidate is considered suitable or not based on the similarity comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
